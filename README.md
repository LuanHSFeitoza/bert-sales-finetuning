
# Fine-Tuning BERT para ClassificaÃ§Ã£o de Texto

Projeto de exemplo para **fine-tuning** do modelo prÃ©-treinado BERT (`bert-base-uncased`) usando a biblioteca Hugging Face Transformers. Ideal para treinar modelos de classificaÃ§Ã£o de texto com datasets customizados em JSONL.

## Estrutura do projeto

meu-projeto-bert/
â”‚
â”œâ”€ notebooks/ # Notebooks do Google Colab
â”œâ”€ scripts/ # Scripts Python para treinamento
â”œâ”€ data/ # Datasets (JSONL)
â”œâ”€ requirements.txt # DependÃªncias do projeto
â””â”€ README.md

## InstalaÃ§Ã£o

Recomendado criar um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # Linux / Mac
venv\Scripts\activate     # Windows
pip install -r requirements.txt

ðŸ”¹ Bloco 1 â€“ Treinamento 

**## Como usar**

1. Ajuste os caminhos para os datasets JSONL (treino.jsonl e teste.jsonl) no script ou notebook.
2. Execute o script ou notebook para treinar o modelo.
3. Verifique os resultados na pasta ./bert-Sales-Challenge-Model-Test.

**## ObservaÃ§Ãµes**

1. Modelo usado: bert-base-uncased
2. Fine-tuning para classificaÃ§Ã£o binÃ¡ria: suporte e venda
3. MÃ©trica de avaliaÃ§Ã£o: accuracy
4. CÃ³digo pensado para ser simples e facilmente adaptÃ¡vel para outros datasets ou tarefas.

ðŸ”¹ Bloco 2 â€“ Login, push para o Hugging Face Hub e uso do modelo

from huggingface_hub import notebook_login
notebook_login()  # Faz login no Hugging Face Hub

trainer.push_to_hub("LuaxSantos/SalesChallengeModel-Finetuning")

from transformers import pipeline

pipe = pipeline("text-classification", model="LuaxSantos/SalesChallengeModel-Finetuning")

resultado = pipe("quero comprar uma nova TV")
print(resultado)
